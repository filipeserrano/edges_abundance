# extraction de la probabilité seuil et des lignes correspondantes
df_filtered_temp <- g2_L[g2_L$ID == i, ]
max_proba <- df_filtered_temp$p_sum[df_filtered_temp$p_sum >= cumulative_thres][1]
# si pas de probabilité cumulée >= 0.95, on prend tout
if(!is.na(max_proba) & max_proba == 1){
df_filtered_temp_selected <- df_filtered_temp[1,]
} else if(!is.na(max_proba) & max_proba != 1){
df_filtered_temp_selected <- df_filtered_temp[df_filtered_temp$p_sum <= max_proba,]
} else {
df_filtered_temp_selected <- df_filtered_temp
}
# ajout des lignes dans le tableau de référence
g2_L_filtered <- rbind(g2_L_filtered, df_filtered_temp_selected)
}
# Chunk 26
head(g2_L_filtered)
# Chunk 27
# INITITALISATION DF
df_fitness <- data.frame(ID = unique(g2_L_filtered$ID), W1 = NA, W2 = NA, W3 = NA, W4 = NA, W5 = NA)
# Chunk 28
# FUNCTION
#' L'idée de la fonction est de compter la longueur de la séquence d'histoire de vie et de déterminer si elle est plus grande que le seuil d'âge (i.e. survie) testé. Si on veut tester la survie jusqu'à 2 ans, alors si la longueur de l'HDV est > 2, on met un "1"
viterbi_survival <- function(df,
p,  # probabilité des histoires de vie
thresh = 1  # seuil d'âge minimale pour la survie
)
{
fitness <- sum(apply(df, 1, function(x)
length(x[x != 3 & x != 0]) > thresh) * p) / sum(p)
return(fitness)
}
# Chunk 29
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 2] <-  viterbi_survival(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # année 1990:2023
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 1)
j = j + 1
}
# Chunk 30
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 3] <-  viterbi_survival(g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2021
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 2)
j = j + 1
}
# Chunk 31
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 4] <-  viterbi_survival(g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2020
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 3)
j = j + 1
}
# Chunk 32
# FUNCTION
viterbi_dominance <- function(df,
p  # probabilité des histoires de vie
)
{
fitness <- sum(apply(df, 1, function(x)
any(x == 2) ) * p) / sum(p)
return(fitness)
}
# Chunk 33
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 5] <-  viterbi_dominance(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2016
p = g2_L_filtered[g2_L_filtered$ID == i, "p"]
)
j = j + 1
}
# Chunk 34
# FUNCTION
viterbi_dominance_duration <- function(df,
p  # probabilité des histoires de vie
)
{
fitness <- sum(apply(df, 1, function(x)
sum(x == 2)) * p) / sum(p)
return(fitness)
}
# Chunk 35
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 6] <-  viterbi_dominance_duration(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2016
p = g2_L_filtered[g2_L_filtered$ID == i, "p"]
)
j = j + 1
}
# Chunk 36
# LOAD PEDIGREE
load("~/marmot-quantitative-genetics/1_DATA_ANALYSIS/3_Data/pedigree_data.rds")
# LOAD CAPTURE MATRIX
load("~/marmot-quantitative-genetics/1_DATA_ANALYSIS/3_Data/mat_capture_completed.rds")
# Chunk 37
# EXTRACTION HDC COMME CHARACTER DANS SORTIE ESURGE ET MAT_CAPTURE_COMPLETED
esurge_hdc_g1 <- apply(g1_O[3:36], 1, function(x) paste0(x, collapse = ""))
esurge_hdc_g2 <- apply(g2_O[3:36], 1, function(x) paste0(x, collapse = ""))
mat_capture_completed_hdc <-  apply(mat_capture_completed, 1, function(x) paste0(x, collapse = ""))
# GET ASSOCIATED ID
df_hdc <- data.frame(individual_id = rownames(mat_capture_completed), hdc_id = NA)
index_g1 <- match(x = mat_capture_completed_hdc, table = esurge_hdc_g1)
index_g2 <- match(x = mat_capture_completed_hdc, table = esurge_hdc_g2)
id_g1 <- g1_O[index_g1, "ID"]
id_g2 <- g2_O[index_g2, "ID"]
sum(is.na(id_g1))
# GET GROUP (capture at birth or adult)
id_juv <- pedigree_data$individual_id[!is.na(pedigree_data$cohort)]
# ASSOCIATE HDC ID WITH INDIVIDUAL ID ACCORDING TO CAPTURE GROUP
i = 0
df_hdc$hdc_id <-
sapply(X = id_g1, FUN = function(x) {
i <<- i + 1
return(ifelse(is.na(x) & !is.na(id_g2[i]), id_g2[i], x))
}
)
#' @Verification : nombre de HDC manquantes (pas incluse dans le modèle)
sum(is.na(df_hdc$hdc_id))  # nombre d'HDC non estimé (à cause des filtres)
# FILTER ONLY INDIVIDUALS CAPTURED AS PUPS
df_hdc_pups <- df_hdc[df_hdc$individual_id %in% id_juv, ]
df_hdc_pups["cohort"] <- pedigree_data$cohort[match(df_hdc_pups$individual_id, pedigree_data$individual_id)] # get cohort per individuals
#' @Verification : nombre de HDC manquantes pour les individus capturés pups
sum(is.na(df_hdc_pups$hdc_id))  # nombre d'HDC non estimé (à cause des filtres)
# Chunk 38
index <- match(df_hdc_pups$hdc_id, df_fitness$ID)
df_fitness_pups <- cbind(df_hdc_pups, df_fitness[index, 2:ncol(df_fitness)])
head(df_fitness_pups)
# Chunk 39
#'@Verification : check si les individus avec une même HDC ont bien la même fitness
head(na.omit(df_fitness_pups[df_fitness_pups$hdc_id == 397,]))
# Chunk 40
# FORMATTING
df_gg <- pivot_longer(df_fitness_pups, cols = 4:8, names_to = "proxy", values_to = "y")
df_gg <-
df_gg |>
mutate(fitness_type =
case_when(
proxy %in% c("W1", "W2", "W3") ~ "Survival",
TRUE ~ "Dominance"
))
# REMOVE NON-SELECTED INDIVIDUALS IF 0 PROBA AT LAST EPSIODE OF SELECTION
row_to_remove <- c()
for(i in 2:5){
previous_episode <- paste0("W",i-1)
actual_episode <- paste0("W",i)
# get id if 0 at the previous episode
id_to_remove <-
df_gg |>
group_by(hdc_id) |>
filter(proxy == actual_episode & y == 0) |>
dplyr::select(hdc_id) |>
unlist()
# remove
index.temp <- which(df_gg$proxy == actual_episode & df_gg$hdc_id %in% id_to_remove)
row_to_remove <- c(row_to_remove, index.temp)
}
df_gg_filter <- df_gg[-row_to_remove,]
# PLOT
plot_survie <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Survival",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#242424"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
plot_dominance <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Dominance",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#CD5555"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
gridExtra::grid.arrange(plot_survie, plot_dominance, nrow = 2)
# Chunk 41
# ON COMPUTE LA MOYENNE PAR ANNEES
mean_W1 <- df_fitness_pups |>
group_by(cohort) |>
filter(cohort != 2023) |>
summarise(mean = mean(W1, na.rm = T)) |>
as.data.frame()
# Chunk 42
par(mfrow = c(1, 1), cex.axis = 1.5, cex.lab = 1.5, pty = "s")
# ON COMPARE AVEC RESULTATS CMR
# viterbi
plot(mean_W1$mean ~ mean_W1$cohort, type = "l", xlab = "Year", ylab = "W1", lwd = 2, ylim = c(0, 1))
mean_W1
# COMPUTE LA FITNESS RELATIVE
index <- match(df_fitness_pups$cohort, mean_W1$cohort)
df_fitness_pups["w1"] <- df_fitness_pups$W1 / mean_W1$mean[index]
data_fitness |>
group_by(annee_capture) |>
summarise(mean = mean(w1, na.rm = T))
# COMPUTE LA FITNESS RELATIVE
index <- match(df_fitness_pups$cohort, mean_W1$cohort)
df_fitness_pups["w1"] <- df_fitness_pups$W1 / mean_W1$mean[index]
df_fitness_pups |>
group_by(annee_capture) |>
summarise(mean = mean(w1, na.rm = T))
df_fitness_pups
df_fitness_pups |>
group_by(cohort) |>
summarise(mean = mean(w1, na.rm = T))
# COMPUTE LA FITNESS RELATIVE
index <- match(df_fitness_pups$cohort, mean_W1$cohort)
df_fitness_pups["w1"] <- df_fitness_pups$W1 / mean_W1$mean[index]
df_fitness_pups |>
group_by(cohort) |>
summarise(mean = mean(w1, na.rm = T))
par(mfrow = c(1, 1), cex.axis = 1.5, cex.lab = 1.5, pty = "s")
# ON COMPARE AVEC RESULTATS CMR
# viterbi
plot(mean_W1$mean ~ mean_W1$cohort, type = "l", xlab = "Year", ylab = "W1", lwd = 2, ylim = c(0, 1))
# ON COMPUTE LA MOYENNE PAR ANNEES
mean_W1 <- df_fitness_pups |>
group_by(cohort) |>
filter(cohort != 2023) |>
summarise(mean = mean(W1, na.rm = T)) |>
as.data.frame()
par(mfrow = c(1, 1), cex.axis = 1.5, cex.lab = 1.5, pty = "s")
# ON COMPARE AVEC RESULTATS CMR
# viterbi
plot(mean_W1$mean ~ mean_W1$cohort, type = "l", xlab = "Year", ylab = "W1", lwd = 2, ylim = c(0, 1))
mean_W1
df_fitness_pups |>
group_by(cohort) |>
filter(cohort != 2023) |>
summarise(mean = mean(W1, na.rm = T)) |>
as.data.frame()
df_fitness_pups
W1
df_fitness_pups |>
group_by(cohort) |>
filter(cohort != 2023) |>
summarise(mean = mean(W1, na.rm = T))
df_fitness_pups
# FORMATTING
df_gg <- pivot_longer(df_fitness_pups, cols = 4:8, names_to = "proxy", values_to = "y")
df_gg <-
df_gg |>
mutate(fitness_type =
case_when(
proxy %in% c("W1", "W2", "W3") ~ "Survival",
TRUE ~ "Dominance"
))
# REMOVE NON-SELECTED INDIVIDUALS IF 0 PROBA AT LAST EPSIODE OF SELECTION
row_to_remove <- c()
for(i in 2:5){
previous_episode <- paste0("W",i-1)
actual_episode <- paste0("W",i)
# get id if 0 at the previous episode
id_to_remove <-
df_gg |>
group_by(hdc_id) |>
filter(proxy == actual_episode & y == 0) |>
dplyr::select(hdc_id) |>
unlist()
# remove
index.temp <- which(df_gg$proxy == actual_episode & df_gg$hdc_id %in% id_to_remove)
row_to_remove <- c(row_to_remove, index.temp)
}
df_gg_filter <- df_gg[-row_to_remove,]
# PLOT
plot_survie <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Survival",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#242424"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
plot_dominance <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Dominance",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#CD5555"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
gridExtra::grid.arrange(plot_survie, plot_dominance, nrow = 2)
#'@Verification : check si les individus avec une même HDC ont bien la même fitness
head(na.omit(df_fitness_pups[df_fitness_pups$hdc_id == 397,]))
par(mfrow = c(1, 2))
hist(
max_proba_cumul_hdc_g1,
xlab = "Max cumulative proba. of HDV",
main = "Group 1",
cex.lab = 1.5,
cex.main = 1.5,
xlim = c(0.90, 1)
)
abline(v = 0.95, col = "red", lty = 2)
hist(
max_proba_cumul_hdc_g2,
xlab = "Max cumulative proba. of HDV",
main = "Group 2",
cex.lab = 1.5,
cex.main = 1.5,
xlim = c(0.90, 1)
)
abline(v = 0.95, col = "red", lty = 2)
# PARAMETERS
cumulative_thres <- 0.95
# GROUP 1
# on associe les ID des HDC aux HDV
g1_L["ID"] <- rep(g1_O$ID, each = 10)
#' On utilise une boucle while qui continue d'ajouter les lignes du df de l'ID donné tant que 0.95 n'est pas atteint
#group1
n_hdc <- nrow(g1_O)
# Boucle FOR par ID, on identifie la première valeur de probabilité cumulée >=0.95, et on sélectionne toutes les HDV précédentes, l'HDV actuelle inclut. Si aucune probabilité cumulue >=0.95, alors on prend les 10 lignes
g1_L_filtered <- matrix(NA, nrow = 0, ncol = ncol(g1_L), dimnames = list(c(), colnames(g1_L)))  # initiliasation
for(i in unique(g1_L$ID)){
# extraction de la probabilité seuil et des lignes correspondantes
df_filtered_temp <- g1_L[g1_L$ID == i, ]
max_proba <- df_filtered_temp$p_sum[df_filtered_temp$p_sum >= cumulative_thres][1]
# si pas de probabilité cumulée >= 0.95, on prend tout
if(!is.na(max_proba) & max_proba == 1){
df_filtered_temp_selected <- df_filtered_temp[1,]
} else if(!is.na(max_proba) & max_proba != 1){
df_filtered_temp_selected <- df_filtered_temp[df_filtered_temp$p_sum <= max_proba,]
} else {
df_filtered_temp_selected <- df_filtered_temp
}
# ajout des lignes dans le tableau de référence
g1_L_filtered <- rbind(g1_L_filtered, df_filtered_temp_selected)
}
# GROUP 2
# on associe les ID des HDC aux HDV
g2_L["ID"] <- rep(g2_O$ID, each = 10)
#' On utilise une boucle while qui continue d'ajouter les lignes du df de l'ID donné tant que 0.95 n'est pas atteint
#group1
n_hdc <- nrow(g2_O)
# Boucle FOR par ID, on identifie la première valeur de probabilité cumulée >=0.95, et on sélectionne toutes les HDV précédentes, l'HDV actuelle inclut. Si aucune probabilité cumulue >=0.95, alors on prend les 10 lignes
g2_L_filtered <- matrix(NA, nrow = 0, ncol = ncol(g2_L), dimnames = list(c(), colnames(g2_L)))  # initiliasation
for(i in unique(g2_L$ID)){
# extraction de la probabilité seuil et des lignes correspondantes
df_filtered_temp <- g2_L[g2_L$ID == i, ]
max_proba <- df_filtered_temp$p_sum[df_filtered_temp$p_sum >= cumulative_thres][1]
# si pas de probabilité cumulée >= 0.95, on prend tout
if(!is.na(max_proba) & max_proba == 1){
df_filtered_temp_selected <- df_filtered_temp[1,]
} else if(!is.na(max_proba) & max_proba != 1){
df_filtered_temp_selected <- df_filtered_temp[df_filtered_temp$p_sum <= max_proba,]
} else {
df_filtered_temp_selected <- df_filtered_temp
}
# ajout des lignes dans le tableau de référence
g2_L_filtered <- rbind(g2_L_filtered, df_filtered_temp_selected)
}
# INITITALISATION DF
df_fitness <- data.frame(ID = unique(g2_L_filtered$ID), W1 = NA, W2 = NA, W3 = NA, W4 = NA, W5 = NA)
# FUNCTION
#' L'idée de la fonction est de compter la longueur de la séquence d'histoire de vie et de déterminer si elle est plus grande que le seuil d'âge (i.e. survie) testé. Si on veut tester la survie jusqu'à 2 ans, alors si la longueur de l'HDV est > 2, on met un "1"
viterbi_survival <- function(df,
p,  # probabilité des histoires de vie
thresh = 1  # seuil d'âge minimale pour la survie
)
{
fitness <- sum(apply(df, 1, function(x)
length(x[x != 3 & x != 0]) > thresh) * p) / sum(p)
return(fitness)
}
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 2] <-  viterbi_survival(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # année 1990:2023
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 1)
j = j + 1
}
df_fitness
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 3] <-  viterbi_survival(g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2021
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 2)
j = j + 1
}
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 4] <-  viterbi_survival(g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2020
p = g2_L_filtered[g2_L_filtered$ID == i, "p"],
thresh = 3)
j = j + 1
}
# FUNCTION
viterbi_dominance <- function(df,
p  # probabilité des histoires de vie
)
{
fitness <- sum(apply(df, 1, function(x)
any(x == 2) ) * p) / sum(p)
return(fitness)
}
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 5] <-  viterbi_dominance(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2016
p = g2_L_filtered[g2_L_filtered$ID == i, "p"]
)
j = j + 1
}
# FUNCTION
viterbi_dominance_duration <- function(df,
p  # probabilité des histoires de vie
)
{
fitness <- sum(apply(df, 1, function(x)
sum(x == 2)) * p) / sum(p)
return(fitness)
}
j = 1
for(i in unique(g2_L_filtered$ID)){
df_fitness[j, 6] <-  viterbi_dominance_duration(df = g2_L_filtered[g2_L_filtered$ID == i, 3:36],  # 1990 à 2016
p = g2_L_filtered[g2_L_filtered$ID == i, "p"]
)
j = j + 1
}
df_fitness
# LOAD PEDIGREE
load("~/marmot-quantitative-genetics/1_DATA_ANALYSIS/3_Data/pedigree_data.rds")
# LOAD CAPTURE MATRIX
load("~/marmot-quantitative-genetics/1_DATA_ANALYSIS/3_Data/mat_capture_completed.rds")
# EXTRACTION HDC COMME CHARACTER DANS SORTIE ESURGE ET MAT_CAPTURE_COMPLETED
esurge_hdc_g1 <- apply(g1_O[3:36], 1, function(x) paste0(x, collapse = ""))
esurge_hdc_g2 <- apply(g2_O[3:36], 1, function(x) paste0(x, collapse = ""))
mat_capture_completed_hdc <-  apply(mat_capture_completed, 1, function(x) paste0(x, collapse = ""))
# GET ASSOCIATED ID
df_hdc <- data.frame(individual_id = rownames(mat_capture_completed), hdc_id = NA)
index_g1 <- match(x = mat_capture_completed_hdc, table = esurge_hdc_g1)
index_g2 <- match(x = mat_capture_completed_hdc, table = esurge_hdc_g2)
id_g1 <- g1_O[index_g1, "ID"]
id_g2 <- g2_O[index_g2, "ID"]
sum(is.na(id_g1))
# GET GROUP (capture at birth or adult)
id_juv <- pedigree_data$individual_id[!is.na(pedigree_data$cohort)]
# ASSOCIATE HDC ID WITH INDIVIDUAL ID ACCORDING TO CAPTURE GROUP
i = 0
df_hdc$hdc_id <-
sapply(X = id_g1, FUN = function(x) {
i <<- i + 1
return(ifelse(is.na(x) & !is.na(id_g2[i]), id_g2[i], x))
}
)
#' @Verification : nombre de HDC manquantes (pas incluse dans le modèle)
sum(is.na(df_hdc$hdc_id))  # nombre d'HDC non estimé (à cause des filtres)
# FILTER ONLY INDIVIDUALS CAPTURED AS PUPS
df_hdc_pups <- df_hdc[df_hdc$individual_id %in% id_juv, ]
df_hdc_pups["cohort"] <- pedigree_data$cohort[match(df_hdc_pups$individual_id, pedigree_data$individual_id)] # get cohort per individuals
#' @Verification : nombre de HDC manquantes pour les individus capturés pups
sum(is.na(df_hdc_pups$hdc_id))  # nombre d'HDC non estimé (à cause des filtres)
index <- match(df_hdc_pups$hdc_id, df_fitness$ID)
df_fitness_pups <- cbind(df_hdc_pups, df_fitness[index, 2:ncol(df_fitness)])
head(df_fitness_pups)
#'@Verification : check si les individus avec une même HDC ont bien la même fitness
head(na.omit(df_fitness_pups[df_fitness_pups$hdc_id == 397,]))
df_fitness_pups
# FORMATTING
df_gg <- pivot_longer(df_fitness_pups, cols = 4:8, names_to = "proxy", values_to = "y")
df_gg <-
df_gg |>
mutate(fitness_type =
case_when(
proxy %in% c("W1", "W2", "W3") ~ "Survival",
TRUE ~ "Dominance"
))
# REMOVE NON-SELECTED INDIVIDUALS IF 0 PROBA AT LAST EPSIODE OF SELECTION
row_to_remove <- c()
for(i in 2:5){
previous_episode <- paste0("W",i-1)
actual_episode <- paste0("W",i)
# get id if 0 at the previous episode
id_to_remove <-
df_gg |>
group_by(hdc_id) |>
filter(proxy == actual_episode & y == 0) |>
dplyr::select(hdc_id) |>
unlist()
# remove
index.temp <- which(df_gg$proxy == actual_episode & df_gg$hdc_id %in% id_to_remove)
row_to_remove <- c(row_to_remove, index.temp)
}
df_gg_filter <- df_gg[-row_to_remove,]
# PLOT
plot_survie <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Survival",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#242424"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
plot_dominance <-
ggplot(data = df_gg_filter[df_gg_filter$fitness_type == "Dominance",], mapping = aes(y = y, fill = fitness_type)) +
scale_fill_manual(values = rev(c("#CD5555"))) +
coord_flip() +
geom_histogram(bins = 30) +
facet_wrap(~proxy, scales = "free") +
theme_classic(base_size = 17) +
theme(legend.title = element_blank())
gridExtra::grid.arrange(plot_survie, plot_dominance, nrow = 2)
# ON COMPUTE LA MOYENNE PAR ANNEES
mean_W1 <-
df_fitness_pups |>
group_by(cohort) |>
filter(cohort != 2023) |>
summarise(mean = mean(W1, na.rm = T)) |>
as.data.frame()
mean_W1
df_fitness_pups
print(c(1,2,3))
